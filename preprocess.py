# This takes sentences generated by generate_sentences.py and converts it to raw text
# While also keeping track of sentence lengths and creating a dictionary of words 
import os 
import nltk 
import json 
import random
import sampling
import csv

def randomize_words_in_sentence(filename):
    data = []
    open("data/txt/incoherent_sentences_randomized_words.txt", 'w') # Clear contents of file 
    randomized_file = open("data/txt/incoherent_sentences_randomized_words.txt", 'a+')

    for line in open("data/json/" + filename, 'r'):
        data.append(json.loads(line))

    for line in data:
        sentence = line['Arg1Raw'] + " " + line['ConnectiveRaw'] + " " + line['Arg2Raw'] + "\n"
        word_sentence = nltk.word_tokenize(sentence.lower())
        random.shuffle(word_sentence)
        for word in word_sentence:
            randomized_file.write(word.encode('ascii', 'ignore') + " ")
        randomized_file.write('\n')


# Stats output file
statistics_file_location = "data/corpus_stats.txt"
open(statistics_file_location, 'w') # Clear contents of file
stats_file = open("data/corpus_stats.txt", 'a+')

def fix_capitalization_arg2(line):
    """
    Capitalizes first letter of the arg2 if it is not already capitalized
    """
    arg2 = line['Arg2Raw'][0]
    if arg2[0].islower():
        temp_arg2 = line['Arg2Raw'][:1].upper() + line['Arg2Raw'][1:]
        arg2 = temp_arg2
    return arg2


def get_original_sentence(coherent_data, incoherent_data_line):
    # Gets the original sentence which was corrupted
    print connect_sentence(incoherent_data_line)
    print connect_sentence(coherent_data[incoherent_data_line['OriginalSentenceIndex']])

    return connect_sentence(coherent_data[incoherent_data_line['OriginalSentenceIndex']])

def connect_sentence(line):
    # Convert to raw text
    # sentence = line['Arg1Raw'] + " " + line['ConnectiveRaw'] + " " + line['Arg2Raw'] + "\n"
    #TODO add option to include connective or not

    sentence = line['Arg1Raw'] + ". " + line['Arg2Raw'] + ". " + "\n"
    # fix_capitalization_arg2(line)
    return sentence

# Build dictionary and convert sentences to raw text
def convert_sentence_to_raw(dictionary, text_properties):
    for filename in os.listdir(os.getcwd() + "/data/json"):
        # Variables for file-specific data
        if filename == ".keep":
            continue
        file_max_sentence_length = 0
        file_num_sentences = 0
        file_dict = {}

        # Import data as a JSON object
        data = []
        for line in open("data/json/" + filename, 'r'):
            data.append(json.loads(line))
            file_num_sentences += 1

        output_file = "data/txt/" + filename[:-5] + ".txt"
        # print(output_file)
        open(output_file, 'w')  # Clear contents of file
        out = open(output_file, 'a+')

        for line in data:
            sentence = connect_sentence(line)
            out.write(sentence.encode('ascii', 'ignore'))

            # Tokenize sentence and build dictionary + corpus stats
            word_sentence = nltk.word_tokenize(sentence.lower())

            # Find maximum sentence length
            if len(word_sentence) > text_properties['max_sentence_length']:
                text_properties['max_sentence_length'] = len(word_sentence)
            if len(word_sentence) > file_max_sentence_length:
                file_max_sentence_length = len(word_sentence)

            # Build dictionary
            for word in word_sentence:
                try:
                    word = word.decode('utf8').encode('ascii', errors='ignore')
                except UnicodeEncodeError:
                    stripped = (c for c in word if 0 < ord(c) < 127)
                    word = ''.join(stripped)
                text_properties['total_words'] += 1
                if word not in dictionary:
                    dictionary[word] = 1
                else:
                    dictionary[word] += 1
                    if dictionary[word] > text_properties['most_frequent_word_freq']:
                        text_properties['most_frequent_word'] = word
                        text_properties['most_frequent_word_freq'] = dictionary[word]

                if word not in file_dict:
                    file_dict[word] = True

        # Get one dataset with words fully randomized TODO look into the filename here, seems wrong
        if filename == 'incoherent_sentences_arg2_diff_sense.json':
            print("Randomizing words in sentence: " + filename)
            randomize_words_in_sentence(filename)

        write_output_stats(filename, file_dict, file_num_sentences, file_max_sentence_length)


def get_random_sample(sample_size, population_size):
    return random.sample(range(1, population_size), sample_size)

def setup_csv(sample_name):
    directory = 'mechanical_turks_input_data'

    file_location = os.path.join(directory, sample_name)
    # Clear contents of file
    f = open(file_location, 'w+')
    f.close()
    # Get txt version of data
    csvfile = open(file_location, 'a+')
    writer = csv.writer(csvfile)
    # Generate header information
    writer.writerow(["Dataset", "CoherentSample", "IncoherentSample"])
    return csvfile, writer

def prepare_sample():

    # Erases and sets up a new CSV file, returning handles
    sample_name = "samples.csv"
    csvfile, writer = setup_csv(sample_name)

    # Loads data for the uncorrupted sentences
    coherent_data = []
    coherent_json_filename = "coherent_sentences.json"
    for line in open(os.path.join("data/json/",coherent_json_filename), 'r'):
        coherent_data.append(json.loads(line))

    for filename in os.listdir(os.getcwd() + "/data/json"):
        # Variables for file-specific data
        if filename in [".keep", coherent_json_filename]:
            continue

        # Loads data for the corrupted sentences
        incoherent_data = []
        for line in open(os.path.join("data/json/",filename), 'r'):
            incoherent_data.append(json.loads(line))

        # Insert from sampling module
        sample_size = 51
        population_size = len(incoherent_data)
        sample_list = get_random_sample(sample_size, population_size)

        counter = 0
        print len(incoherent_data)
        for i, line in enumerate(incoherent_data):
            if i in sample_list:
                counter += 1
                incoherent_sentence = connect_sentence(line).encode('ascii', 'ignore')
                coherent_sentence = get_original_sentence(coherent_data, line).encode('ascii', 'ignore')
                writer.writerow([filename, incoherent_sentence, coherent_sentence])
                sample_list.remove(i)
                if len(sample_list) == 0:
                    break

    csvfile.close()


def write_output_stats(filename, file_dict, file_num_sentences, file_max_sentence_length):
    # Output File Stats
    stats_file.write(filename + " stats:\n")
    stats_file.write("# words: " + str(len(file_dict.keys())) + "\n")
    stats_file.write("# sentences: " + str(file_num_sentences) + "\n")
    stats_file.write("Max sentence length: " + str(file_max_sentence_length) + "\n")

def write_corpus_statistics(text_properties):
    print("Writing statistics file: " + statistics_file_location)
    # Output corpus stats
    stats_file.write("Total words: " + str(text_properties['total_words']) + "\n")
    stats_file.write("Most frequent word: " + text_properties['most_frequent_word'] + "\n")
    stats_file.write("Most frequent word frequency: " + str(text_properties['most_frequent_word_freq']) + "\n")
    stats_file.write("Unique terms in dictionary: " + str(len(dictionary.keys())) + "\n")
    stats_file.write("Max sentence length: " + str(text_properties['max_sentence_length']) + "\n")


def map_terms_to_integers():
    # Output dictionary and create mapping of terms to integers
    index = 1
    open("data/dictionary.txt", 'w') # Clear contents of file
    dict_file = open("data/dictionary.txt", 'a+')
    # Used for padding sentences to max_sentence_length
    mapped_dictionary["<pad>"] = 0
    dict_file.write("0 <pad> -1\n")

    # Print all terms from dictionary and map them to integers in mapped_dictionary
    for key in sorted(dictionary.keys()):
        mapped_dictionary[key.lower()] = index
        entry = str(index) + " " + key + " " + str(dictionary[key]) + "\n"
        dict_file.write(entry)
        index += 1

if __name__ == '__main__':

    dictionary = {}
    mapped_dictionary = {}

    text_properties = {
        'max_sentence_length': 0,
        'most_frequent_word': "",
        'most_frequent_word_freq': 0,
        'total_words': 0
    }

    print("Converting to raw text")
    # convert_sentence_to_raw(dictionary, text_properties)
    # write_corpus_statistics(text_properties)

    # sampling.sample_for_crowdflower()
    prepare_sample()
    # sampling.sample_for_mechanical_turks()